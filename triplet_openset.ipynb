{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "import glob\n",
    "from PIL import Image\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from typing import Callable, Sequence\n",
    "from utility import img_transform, EmbeddingHead\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/pblock-32965-idx_280x175\"\n",
    "NUM_TRAIN = 6593 # 20%\n",
    "NUM_TEST = 26372 # 80%\n",
    "NUM_TOTAL = 35912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"metadata.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PalletTripletDataset():\n",
    "    def __init__(self, img_df, transform=None, target_transform=None):\n",
    "        self.img_df = img_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.pallet_ids = img_df[\"target\"].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pallet_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.pallet_ids[idx]\n",
    "        rows = self.img_df.loc[self.img_df[\"target\"] == pid]\n",
    "        \n",
    "        imgs = []\n",
    "\n",
    "        for i in range(len(rows)):\n",
    "            img = Image.open(rows.iloc[i][\"path\"]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            imgs.append(img)\n",
    "        \n",
    "        return torch.stack(imgs), torch.ones(len(imgs)) * idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selector = (df[\"target\"] < NUM_TRAIN)\n",
    "trainset = PalletTripletDataset(df.loc[train_selector], transform=img_transform, target_transform=lambda x: torch.tensor(x, dtype=torch.long))\n",
    "print(\"Trainset: \", len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    dot_product = torch.matmul(embeddings, torch.transpose(embeddings, 0, 1))\n",
    "    square_norm = torch.diagonal(dot_product)\n",
    "    distances = torch.unsqueeze(square_norm, 0) - 2.0 * dot_product + torch.unsqueeze(square_norm, 1)\n",
    "    distances = torch.maximum(distances, torch.tensor(0.0))\n",
    "\n",
    "    if not squared:\n",
    "        mask = torch.eq(distances, 0.0).type(torch.float)\n",
    "        distances = torch.sqrt(distances)\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    mask = torch.zeros((len(labels), len(labels)))\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if(i != j and labels[i] == labels[j]):\n",
    "                mask[i,j] = 1\n",
    "    return mask\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    mask = torch.zeros((len(labels), len(labels)))\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if(labels[i] != labels[j]):\n",
    "                mask[i,j] = 1\n",
    "    return mask\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels).type(torch.float).to(device)\n",
    "    anchor_positive_dist = torch.multiply(mask_anchor_positive, pairwise_dist)\n",
    "    hardest_positive_dist = torch.max(anchor_positive_dist, dim=1, keepdim=True).values\n",
    "\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels).type(torch.float).to(device)\n",
    "    max_anchor_negative_dist = torch.max(pairwise_dist, dim=1, keepdim=True).values\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "    hardest_negative_dist = torch.min(anchor_negative_dist, dim=1, keepdim=True).values\n",
    "\n",
    "\n",
    "    triplet_loss = torch.maximum(hardest_positive_dist - hardest_negative_dist + margin, torch.tensor(0.0))\n",
    "    triplet_loss = torch.mean(triplet_loss)\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet50\", pretrained=True)\n",
    "net.fc = EmbeddingHead()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[20, 40], gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 80\n",
    "SAVE_PATH = \"model/\"\n",
    "\n",
    "net.to(device)\n",
    "net.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        x, y = data\n",
    "        x = torch.flatten(x, end_dim=1)\n",
    "        y = torch.flatten(y, end_dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        out = net(x.to(device))\n",
    "        loss = batch_hard_triplet_loss(y.to(device), out, margin=0.5, squared=False)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 199:.3f}')\n",
    "            running_loss = 0.0\n",
    "    lr_scheduler.step()\n",
    "    torch.save(net.state_dict(), os.path.join(SAVE_PATH, \"model_triplet_256x128_\" + str(date.today()) + \".pth\"))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69cf509240d94a37c1094653e766edea7cfd00f05bbee75c76ffbfcae4e4ebc8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('torch_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
